# Практическая работа №1
### 1. Программа вычисления весов
#### Код
`generate_weights.cpp`:
```
#include <iostream>
#include <vector>
#include <random>
#include <fstream>
#include <iomanip>
#include <algorithm>
#include <cmath>

// --- Matrix struct and functions ---
struct Matrix { size_t r,c; std::vector<double> d; Matrix(size_t R=0,size_t C=0):r(R),c(C),d(R*C,0.0){} void randomize(unsigned int s){std::mt19937 g(s);std::uniform_real_distribution<> u(-1.,1.);for(auto&v:d)v=u(g)*std::sqrt(2.0/(r+c));} double& at(size_t R,size_t C){return d[R*c+C];} const double& at(size_t R,size_t C)const{return d[R*c+C];} };
Matrix multiply(const Matrix&a,const Matrix&b){Matrix r(a.r,b.c);for(size_t i=0;i<a.r;++i)for(size_t j=0;j<b.c;++j)for(size_t k=0;k<a.c;++k)r.at(i,j)+=a.at(i,k)*b.at(k,j);return r;}
Matrix add_bias(const Matrix&a,const Matrix&b){Matrix r=a;for(size_t i=0;i<a.r;++i)for(size_t j=0;j<a.c;++j)r.at(i,j)+=b.at(0,j);return r;}
Matrix apply_relu(const Matrix&m){Matrix r=m;for(auto&v:r.d)v=std::max(0.0,v);return r;}
Matrix relu_derivative(const Matrix&m){Matrix r=m;for(auto&v:r.d)v=(v>0)?1.0:0.0;return r;}
Matrix transpose(const Matrix&m){Matrix r(m.c,m.r);for(size_t i=0;i<m.r;++i)for(size_t j=0;j<m.c;++j)r.at(j,i)=m.at(i,j);return r;}
Matrix hadamard(const Matrix&a,const Matrix&b){Matrix r(a.r,a.c);for(size_t i=0;i<a.d.size();++i)r.d[i]=a.d[i]*b.d[i];return r;}
Matrix sum_rows(const Matrix&m){Matrix r(1,m.c);for(size_t j=0;j<m.c;++j)for(size_t i=0;i<m.r;++i)r.at(0,j)+=m.at(i,j);return r;}

// Функция для нормализации значения в диапазон [-1, 1]
double normalize(double val, double min, double max) { return 2.0 * (val - min) / (max - min) - 1.0; }

int main() {
    const size_t NUM_POINTS = 3, INPUT_SIZE=NUM_POINTS*2, HIDDEN1_SIZE=32, HIDDEN2_SIZE=16, OUTPUT_SIZE=2;
    Matrix W1(INPUT_SIZE,HIDDEN1_SIZE),B1(1,HIDDEN1_SIZE),W2(HIDDEN1_SIZE,HIDDEN2_SIZE),B2(1,HIDDEN2_SIZE),W3(HIDDEN2_SIZE,OUTPUT_SIZE),B3(1,OUTPUT_SIZE);
    W1.randomize(1);B1.randomize(2);W2.randomize(3);B2.randomize(4);W3.randomize(5);B3.randomize(6);

    double learning_rate=0.001; int steps=80000; int batch_size=128;
    std::mt19937 gen(1337);
    const double M_B_MIN=-5.0, M_B_MAX=5.0; // Диапазон для m и b
    const double X_MIN=-10.0, X_MAX=10.0;   // Диапазон для x
    const double Y_MIN=M_B_MIN*X_MIN+M_B_MIN, Y_MAX=M_B_MAX*X_MAX+M_B_MAX; // Примерный диапазон для Y
    std::uniform_real_distribution<>m_b_dist(M_B_MIN,M_B_MAX), x_dist(X_MIN,X_MAX);

    std::cout << "Обучаем сеть с нормализацией данных..." << std::endl;
    for (int step=0; step<steps; ++step) {
        Matrix X_batch(batch_size,INPUT_SIZE), Y_batch(batch_size,OUTPUT_SIZE);
        for (int i=0; i<batch_size; ++i) {
            double true_m=m_b_dist(gen), true_b=m_b_dist(gen);
            for (int p=0; p<NUM_POINTS; ++p) {
                double x=x_dist(gen), y=true_m*x+true_b;
                X_batch.at(i,p*2+0)=normalize(x,X_MIN,X_MAX); // Нормализуем входы
                X_batch.at(i,p*2+1)=normalize(y,Y_MIN,Y_MAX);
            }
            Y_batch.at(i,0)=normalize(true_m,M_B_MIN,M_B_MAX); // Нормализуем выходы
            Y_batch.at(i,1)=normalize(true_b,M_B_MIN,M_B_MAX);
        }
        Matrix Z1=add_bias(multiply(X_batch,W1),B1),A1=apply_relu(Z1);
        Matrix Z2=add_bias(multiply(A1,W2),B2),A2=apply_relu(Z2);
        Matrix Z3=add_bias(multiply(A2,W3),B3),Y_pred=Z3;
        Matrix error(batch_size,OUTPUT_SIZE); for(size_t i=0;i<error.d.size();++i)error.d[i]=Y_pred.d[i]-Y_batch.d[i];
        Matrix dZ3=error,dW3=multiply(transpose(A2),dZ3),dB3=sum_rows(dZ3),dA2=multiply(dZ3,transpose(W3));
        Matrix dZ2=hadamard(dA2,relu_derivative(Z2)),dW2=multiply(transpose(A1),dZ2),dB2=sum_rows(dZ2),dA1=multiply(dZ2,transpose(W2));
        Matrix dZ1=hadamard(dA1,relu_derivative(Z1)),dW1=multiply(transpose(X_batch),dZ1),dB1=sum_rows(dZ1);
        double N=static_cast<double>(batch_size);
        for(size_t i=0;i<W1.d.size();++i)W1.d[i]-=learning_rate*dW1.d[i]/N; for(size_t i=0;i<B1.d.size();++i)B1.d[i]-=learning_rate*dB1.d[i]/N;
        for(size_t i=0;i<W2.d.size();++i)W2.d[i]-=learning_rate*dW2.d[i]/N; for(size_t i=0;i<B2.d.size();++i)B2.d[i]-=learning_rate*dB2.d[i]/N;
        for(size_t i=0;i<W3.d.size();++i)W3.d[i]-=learning_rate*dW3.d[i]/N; for(size_t i=0;i<B3.d.size();++i)B3.d[i]-=learning_rate*dB3.d[i]/N;
        if(step%5000==0){double loss=0;for(const auto&e:error.d)loss+=e*e;std::cout<<"Шаг "<<std::setw(5)<<step<<", Потери: "<<loss/N<<std::endl;}
    }
    std::ofstream outfile("network_weights.txt"); outfile << std::fixed << std::setprecision(17);
    outfile << X_MIN << " " << X_MAX << "\n";
    outfile << Y_MIN << " " << Y_MAX << "\n";
    outfile << M_B_MIN << " " << M_B_MAX << "\n";
    auto save_matrix=[&](const Matrix& m){outfile<<m.r<<" "<<m.c<<"\n";for(size_t r=0;r<m.r;++r){for(size_t c=0;c<m.c;++c){outfile<<m.at(r,c)<<(c==m.c-1?"":" ");}outfile<<"\n";}};
    save_matrix(W1);save_matrix(B1);save_matrix(W2);save_matrix(B2);save_matrix(W3);save_matrix(B3);
    outfile.close(); std::cout<<"Веса и параметры нормализации сохранены."<<std::endl;
    return 0;
}
```

#### Компиляция
```
g++ generate_weights.cpp -o generate_weights.exe -std=c++17 -O2
```
Либо через кодировку CP1251 (рекомендовано):
```
g++ generate_weights.cpp -o generate_weights.exe -std=c++17 -O2 -finput-charset=CP1251 -fexec-charset=CP866
```
#### Запуск
Программа автоматически сгенерирует веса и сохранит в файл `network_weights.txt`.
### 2. Веса
`network_weights.txt`:
```
-10.00000000000000000 10.00000000000000000
45.00000000000000000 55.00000000000000000
-5.00000000000000000 5.00000000000000000
6 32
0.12410500484368164 0.25722885587057720 -0.17107459805489367 0.44849080908472871 -0.12116249732434736 -0.30968521731420551 -0.05460825170984757 0.07787037516465128 0.19998589375301742 0.14234574229424699 -0.22818547562989303 -0.01920679441100187 0.11279813189127626 -0.08856028822249426 0.08380098787902564 0.18993220972348612 -0.24168446671224184 -0.26899398359025478 0.33918227840413873 0.09856546706365922 0.22250349839931088 0.05199421668768711 -0.23629461588297804 0.17910416082214353 0.17935166395989921 0.34483735791821907 0.32497340780200334 -0.16527431640213860 -0.15259850312390924 -0.05497820384619447 -0.01043983998743738 0.18014040651936661
0.04760723857183977 -0.16345900914970851 -0.06541519486879936 -0.16193511408241629 0.12997613154581769 -0.03928148118598267 -0.15669452236934203 0.05690834819212454 0.07362374235482903 -0.12109800858832076 -0.06289510221593769 -0.05423895661171398 -0.14010292697548568 -0.05893564449211779 -0.14575801892525334 0.18507601257960898 -0.20971845137130340 0.02909310575180880 -0.07274218293600225 0.22898579521922743 0.16441885388016186 -0.10487332837218481 0.08922084492610530 -0.07363900485937762 0.04232836806802519 0.10186145166561221 -0.05290077453150952 -0.07317785748602293 -0.01433866149489702 0.04080328236040243 -0.09156894377747930 0.07312981229338669
0.37559194255196982 -0.29528500406752684 -0.15395272546336444 0.00164626803471889 -0.21580333418064984 0.48773904042943855 0.00011007684675240 -0.02386807359245757 0.18006501130412814 -0.10285824153690380 0.14365562341067920 0.09402509958925459 -0.05167447612104956 0.15123543209560697 0.09778065208243010 -0.17146455755464063 -0.00049949248150900 -0.04936740722876969 -0.04168273187923355 0.11374023053468803 -0.28331050913817979 0.19229928212871580 0.36435913939888703 -0.34591049897941073 0.06461925385722114 -0.30073427690056925 0.19763897820948773 -0.24933847673253010 -0.26406267548854545 0.39035212106227268 -0.07749183759741256 0.16768579192837590
-0.02732051543869441 -0.06967326739627068 0.13032010161268048 -0.18535790565016730 0.03596107893599337 -0.21020151745037610 0.04395169826674424 -0.01332930976106847 0.12126260950631849 0.06616369611448469 -0.10782434234170925 0.22855547647015301 0.00592197549156747 -0.18079121905759812 0.00810096124930756 0.11139041525776293 -0.17615368429739559 0.00480306915933066 -0.05942008763473325 0.05907938557260763 -0.07685884293026653 -0.21113482331945826 -0.04704701215263026 -0.17323804224582198 -0.06475538797432022 -0.18092149498793017 -0.02676929489374261 -0.15272437645526890 -0.03049538134883476 -0.08545740473726708 0.14717692827413864 -0.09787680231139936
-0.41674684426258435 -0.28943031373191741 0.04544028938218046 -0.08629899707817565 -0.06070891573001346 0.19302215995911273 0.04423029481805293 0.09973315097479581 0.05196152390317103 -0.13191516102038436 0.15817314403740798 -0.03903842891825585 0.12966273125919020 -0.10154547853713027 -0.28313507737348992 0.20059019319628574 0.35318782542578558 0.35573201846980207 -0.02930181092630429 -0.20300482447381815 0.23020389175789763 -0.21928540449823272 -0.24621407180161400 -0.15256595383994823 -0.21452064810410187 -0.19910993617203046 -0.10865365934091176 -0.20408479848508126 0.55474919691227786 -0.14874692785973254 -0.32112531187924909 0.04071431751469416
-0.04059204921798354 -0.08121918835832379 0.23218920826655207 0.10787878973706849 0.11897435052113782 -0.00301158426567352 -0.22410309791944774 0.14403049027492551 0.12030401788005750 0.03019797327887804 -0.04353550634065622 -0.13544144103183811 -0.14983014510389378 0.02748198461672985 -0.11315549809575164 0.19107559596925988 -0.09702020208850079 -0.03837054351628568 0.11063601709529937 -0.04375810737792271 -0.21449591868298329 -0.17635835812030309 -0.10923494443729838 0.01708607019495147 -0.18804119939592004 -0.15117590022590915 0.04068591731778875 0.01742526182241948 0.05606939387367746 0.02307075079360076 -0.10037877623745846 -0.01841869786270850
1 32
-0.06818881711020129 0.22674928709949224 0.21497300140893832 0.01452562905733068 -0.08828403545207733 -0.05964168672981928 0.02815339780214280 -0.18713735467026849 -0.00714134333392385 0.23063620530543363 0.14641504702413893 0.09942692978263716 -0.02389058615919406 0.03148124599571044 0.19373738965773532 -0.11891413953007721 -0.07856512578748466 -0.04344963220046950 -0.15131766245936895 0.22485732242784034 -0.11665359414157038 -0.16021314955980720 -0.06862783911209838 -0.26121718058707100 0.03312926640819674 0.03512881114757975 -0.26213290989548405 0.22659765873395624 0.11326878750661275 -0.18017602091893312 -0.08997248105659562 -0.09275755354480961
32 16
-0.17636579656221785 0.12654435326760821 -0.32078256701385743 -0.10062005898860561 0.11924312801366004 -0.19770143471329960 -0.27841968628194391 -0.24858770292124224 -0.23031714045211465 0.07866536659397867 -0.14477703105883796 -0.00497198959619649 -0.11575105690257464 0.06541508478236625 -0.00381165471286090 0.12730614121735695
0.10337958481739121 0.17831731000539777 0.15179997963797320 -0.08997134062903440 0.05804104338174379 0.09617558256815717 -0.12030534783315225 -0.02175185735370251 0.16321005601078012 0.13228428891995858 -0.09896539019694027 0.17153936482692214 0.07325495458700480 -0.31079238871670267 -0.18045218438319241 -0.05649630019664563
0.12350426169618009 -0.04114581945123225 -0.03801304876488720 -0.03440142707835404 0.11454650245112898 0.08736728330683199 -0.19787475987684802 -0.07510176956923832 0.01301386539322329 0.10136813246561846 0.04475367347195901 -0.11618306603898360 0.06042865693643037 0.07270154050331352 0.14680416375480906 0.12127559017830851
-0.07261276700195761 -0.17656007896365339 0.14910731558471915 -0.25336513999261401 0.25178223623251766 0.03934936314786894 -0.00944116107243065 -0.02922842847166196 -0.07061189444442829 -0.01267595466236049 0.12588835452618616 -0.06139315036754429 0.12999951734156381 -0.15157816992125742 -0.01633408875987895 0.18897480495971197
0.00886479821458214 -0.14590791948493725 -0.08064134903011044 0.12535137874492655 -0.07375731268201058 -0.19545916346323464 -0.00716631049295231 -0.19346622222205023 0.00233726245335184 0.17065894183251562 -0.11963057411183603 0.17389266825692959 -0.09090748455334247 -0.13225683298207183 0.20016462205121477 0.07865104193830937
0.08341401766372775 -0.16252650784094622 -0.42520866252085077 0.17197419203529007 -0.22684558102842217 -0.17852361674611381 0.01419106915873724 -0.03808842632026225 0.04956599655058105 0.02880232109867321 0.15558780820332105 0.13040381027491055 -0.06682775220861903 0.26579825728921053 0.14092030890011339 0.05457638374946576
-0.09450825140481423 0.13718795750996923 0.01613518928726933 0.15451119141386549 -0.16431046172682656 -0.07967548024888099 0.15313563585838399 0.08846706036765067 -0.09686845039050293 -0.01315057926677211 0.00103295646277086 -0.10257055347052701 -0.09715388051101613 -0.01436211063958622 0.12071278234623502 -0.00916208523051713
0.02948769721101754 -0.06433193426513899 0.04475761586908692 -0.15488617895344819 -0.18822766546585190 0.20413498292435522 -0.09473576475883522 -0.09919502495436724 -0.09107759810425115 -0.10135109292409114 0.16237546230386718 0.13683704813019665 -0.14439079755170178 0.20227118814214767 -0.05890529650792035 0.01931376028807171
-0.00342020170183093 -0.15119133160769321 0.14675100316373343 -0.11503692459067086 0.13259406433962712 0.20342323383548816 0.20053613866291881 -0.13990100843034384 -0.01209804288300537 0.08468469673132745 -0.06531730750619467 0.08012891040597789 0.10141202697006228 0.12961429663850252 0.14194055267054345 -0.17080916661660955
-0.08502145978323872 0.02387148086623092 0.24351953042798208 -0.24426237139841472 -0.00437483286635803 0.04784742124856133 0.03676092820138888 -0.10236216589635296 0.17557820148978198 0.00105241787144282 0.08317638436560947 0.04529027809628042 0.04227843664677770 0.17953557914203863 -0.16040207008562068 0.15383039487200495
-0.09144437057093437 -0.16292604927122953 -0.21421562684264459 0.14771616201274360 -0.23162140102444209 0.04803992257216720 -0.17658524162426903 0.14250345296728073 -0.00383230493931235 0.12262693875328604 0.08963327984044377 0.03154040256345673 -0.11505684299064060 0.09968142313089989 0.04313755497267108 0.10555439383570484
0.11710435757783487 -0.05481977538214524 -0.10259732575008576 -0.18669078442423662 -0.00687251939227587 0.14708439298220866 0.14029147606272407 0.05875536573233986 0.02784379652741471 -0.14951938112881133 -0.19510356679392815 -0.04478011877168932 -0.06678264120131747 0.18374499724005358 -0.03906813730525827 -0.00924303084983825
-0.00003301504860158 -0.06993180580759581 -0.05877250805936551 0.14121414743906602 0.13591092769758356 -0.02617767626721942 0.14187701277495612 -0.09419833319949089 -0.11117829957317328 -0.11734817472182955 -0.03255106673735753 -0.19283302533456578 0.18716233282388653 -0.02934105999392998 0.00892409649389743 0.20904986064697542
-0.09234019940965878 -0.03225783891791964 -0.11544813930607355 0.13437255250681257 -0.00305592251508280 -0.01942940115372419 -0.18216470809651661 -0.19293780782970849 -0.10808301840465723 -0.19745401041073773 -0.09200386778822786 0.26219009541590399 -0.09786537532303639 -0.06617090887851243 -0.11055123158535922 0.13225471710412270
-0.04178269466044587 0.07665864333975647 0.04191304488819923 -0.18802864343529127 0.12155194190106804 -0.06849783433180609 0.04736848035788398 0.11156338747632207 0.10019442130415854 0.10690743431881629 0.05635985287580061 0.20098709379147423 -0.21920488703190710 0.08589443657243957 -0.17276635580806027 0.09533871807978800
-0.19391148860836488 -0.17723287769204982 -0.07094037622694849 0.03426339540931628 -0.16305225111482921 0.00078520167827084 0.12287810250804047 -0.17539798392740105 0.13835045205319932 0.07767997161914481 -0.15343702380241994 0.01370437933877947 0.14061989068623446 0.04791557511042440 0.20188428922654758 -0.06769335180154948
0.11899490537455608 -0.05244424907301302 0.01490970581112544 0.22484648386064163 -0.29930165688634297 0.10244635883241079 0.15759468187715592 0.01962048891663713 -0.11819132484067159 -0.12339743649876959 0.02710012390801909 -0.31601776213863914 -0.19742336820747564 0.17973572988080541 0.08723123191631604 -0.23102646644552183
-0.12665237405904486 0.05551775229827045 0.22916683350309822 0.19375830820397841 -0.04035015639342134 -0.18306806471369255 0.01626007821219085 0.29692741381905180 0.19146596469313976 -0.12023366571800362 -0.10949875484884533 0.21663033711652896 0.09572751290933035 -0.03040160560127121 0.19312290933696263 0.19689764103578122
-0.18867897279314938 0.01338259354948635 0.25175579176927781 0.01173863738035500 0.16169745270932281 0.02509503037147532 0.13352734172317324 0.12096704221086037 0.06074717724399880 -0.18444260711351804 -0.09286283028369090 0.10299517578538163 0.02995458434690454 0.11613417674079567 0.01358372217427418 0.02888969491468285
0.08570709975679335 -0.07610905949472431 -0.05370230051205377 0.01909800472440781 -0.10173358110213133 0.08274862522693778 -0.02962806794065805 -0.05896506477867242 0.16848841024758376 -0.18207898144151058 0.09081164891190888 0.04030024140336394 -0.03657766752704307 -0.13574621291947439 -0.05535424486178823 -0.10045634805945369
0.04293528098064631 0.16431815590595036 0.21049383739682023 0.08644634993114830 0.11357902426711040 0.17889351596025738 0.12371475799192713 0.01405382181634210 -0.01470678135279127 -0.10787490505197364 -0.20010011354925245 -0.36026824815304032 -0.00066213800598100 -0.13542079548220734 0.06140298825952138 0.07273576358180778
-0.18700594272935953 -0.07808388221362067 -0.07105505497762224 -0.07826946725394467 0.05689540539785010 -0.05991822214315656 -0.02607101747890796 0.11855892077865389 -0.14018330760845352 0.04074620025891013 -0.18956676537604469 0.03701846876738294 0.15483454890195886 0.16413309174150917 0.11363317059543153 -0.12452492489619224
-0.16884734333377269 -0.08549025247471688 -0.37378842156121711 -0.14918337670085285 0.13228848802123067 0.03328171284666213 -0.05395083549025583 0.21629256312499021 0.09544900896276787 -0.17196661190146392 -0.11403450340923743 0.28338340867835793 0.15544875505681277 0.19471782247113031 -0.11159152221964075 0.01590370647959823
-0.17717966275015573 -0.15173590662666867 0.09695650168385044 -0.20717235294914579 0.29757921325243691 0.10158571986726099 -0.04341972852785698 0.00208320301820433 -0.10758655943978304 0.10097813042601626 0.13346745164603285 -0.11657254595475282 -0.00986846885330105 -0.23747792945112758 -0.20497451432368968 -0.22387112361392031
-0.01702132448411994 -0.07072782576686351 0.11335767919093222 0.11308221297528122 0.14853600199271899 -0.22040867456913937 -0.15627005573856689 -0.13658514943409256 0.08539829268279127 -0.05395960630353007 0.02605512017413839 0.21702329016792948 0.10303260538182191 -0.03801544732028498 -0.18618389623731066 0.02871470592461754
0.06875623003711523 -0.18956636614582584 0.07191182526109016 -0.22072535497137405 0.16915934625351248 0.18457588752985252 0.01896146952064083 0.09134053566730684 0.05781507050785191 -0.07618449315438749 -0.17404291614869150 -0.03105178380644096 -0.15200989889907546 -0.29404595133696465 -0.12386689205008432 0.07266730149945916
0.01631546640298935 0.06989883378022219 0.17636498794886663 0.12487130886416432 -0.13984982905001098 -0.05881508277070925 -0.14918966927082530 0.29137461536014170 -0.23467358034899280 0.15996217380148378 -0.14088651001857408 -0.02554757746175212 0.17925700520459864 -0.13264794849909489 -0.08578494250575712 -0.07230997497731671
-0.07176756023907656 0.18281655239401814 0.10098986106260742 -0.10553474353721608 -0.15067974493655281 0.10295512981545414 -0.03698041735463659 -0.01188128433353673 0.26349359652928495 0.09028289925344894 -0.13447258850463081 0.09792427305602849 0.02126163327754898 -0.04624901031743613 0.11502907856386775 -0.12082984737931544
0.14444190586257794 -0.00681359831376639 -0.36180104651395822 -0.14774480035025961 0.10866725341234235 -0.04463090747936572 0.07132111029540716 -0.37823674839118082 0.00333828695225050 0.07447430136419572 0.05981373766724592 -0.35550030254077319 -0.07765672553825669 -0.03805820586826187 0.14224101279686510 0.03175260658016718
-0.19180231310026480 0.17891011271616905 -0.11090467713098015 0.22341919199984264 -0.20388588117773826 -0.06249216858758011 -0.00319769255601085 0.22365161012301518 0.08523400137938074 -0.09444165287705257 0.18814702938471445 0.20506679101698796 -0.10283792282679298 0.17158006582837793 0.00744952512617070 -0.00801606387355090
0.10072362046402113 0.15937025202513946 0.06362734441290928 -0.21110998695538583 0.10909704935045823 -0.00105544598821112 -0.09436802425080155 -0.07767671460997504 -0.02915106342479946 0.14996842494862070 0.18038283222772411 -0.09433038863876420 0.14049481719121215 0.06881503499497588 -0.09656636521316612 -0.15340241738422508
0.05429532564335438 -0.03865323903896899 0.09813765842083881 -0.01478441834284845 0.15370326110520069 -0.05703426660920238 0.19263435340477092 0.14435193190291240 -0.10050770019242278 -0.14239485342947120 0.13837631676513715 0.12773143311119955 -0.16222553207864854 -0.07678536475864235 0.06888453825443913 0.02138728097806756
1 16
0.26615596799596830 -0.21367830206757155 0.20809344879562131 -0.02638590689675106 0.03182147445644459 -0.26385230513229191 -0.12019317621821771 0.03565061561881633 0.73153958754233528 0.09128201698816142 -0.33945802254226093 0.17736180317989389 -0.09883313038138775 0.20334485539262023 -0.06993631350937045 0.42980448171491203
16 2
-0.29283562955721992 0.22266211812463907
-0.10066172494587895 0.31400013617274358
-0.85189194200834828 -0.04824286377497453
-0.54796568904992216 0.06855332312736877
0.67778323389712258 0.03659705682276623
0.33850536842704865 0.03800306723261591
-0.27118817028955677 0.03393193160155815
-0.53533850854280152 -0.36599943802721591
-0.56221775539819230 0.60012392760818201
0.01073417100262941 -0.05172998849690197
0.15458073243497705 -0.32361283288867848
-0.76584598755138755 0.14204653792254249
0.22233916765812900 0.25998773231840738
0.63603810224772273 0.00883186198076889
-0.25958933332089784 0.32548981082834755
0.01702571566009466 0.44875210718505998
1 2
0.66791789322612816 0.06552873231821339

```
### 3. Программа визуализации
#### Код
`visualize_approximation.cpp`:
```
#include <iostream>
#include <vector>
#include <string>
#include <sstream>
#include <fstream>
#include <stdexcept>
#include <utility>
#include <cmath>
#include <algorithm>
#include <cstdint>
#include <thread>
#include <mutex>
#include <optional>
#include <iomanip>
#define SDL_MAIN_HANDLED
#include <SDL2/SDL.h>

struct Matrix { size_t r,c; std::vector<double> d; Matrix(size_t R=0,size_t C=0):r(R),c(C),d(R*C,0.0){} double& at(size_t R,size_t C){return d[R*c+C];} const double& at(size_t R,size_t C)const{return d[R*c+C];}};

class NeuroProcessor {
private:
    Matrix W1,B1,W2,B2,W3,B3;
    double x_min, x_max, y_min, y_max, mb_min, mb_max;
    static Matrix add_bias(const Matrix&a,const Matrix&b){Matrix r=a;for(size_t i=0;i<a.r;++i)for(size_t j=0;j<a.c;++j)r.at(i,j)+=b.at(0,j);return r;}
    static Matrix multiply(const Matrix&a,const Matrix&b){Matrix r(a.r,b.c);for(size_t i=0;i<a.r;++i)for(size_t j=0;j<b.c;++j)for(size_t k=0;k<a.c;++k)r.at(i,j)+=a.at(i,k)*b.at(k,j);return r;}
    static Matrix relu(const Matrix&m){Matrix r=m;for(auto&v:r.d)v=std::max(0.0,v);return r;}
    double normalize(double val, double min, double max) const { return 2.0 * (val - min) / (max - min) - 1.0; }
    double denormalize(double val, double min, double max) const { return (val + 1.0) / 2.0 * (max - min) + min; }
public:
    void load_weights(const std::string& filename) {
        std::ifstream infile(filename); if(!infile)throw std::runtime_error("Нет файла: "+filename);
        infile >> x_min >> x_max >> y_min >> y_max >> mb_min >> mb_max;
        auto read=[&](Matrix&m){infile>>m.r>>m.c;m.d.resize(m.r*m.c);for(auto&v:m.d)infile>>v;};
        read(W1);read(B1);read(W2);read(B2);read(W3);read(B3);
        std::cout << "Веса для сети-регрессора успешно загружены." << std::endl;
    }
    std::pair<double, double> process(const std::vector<std::pair<double, double>>& points) {
        Matrix input(1, W1.r);
        for(size_t i=0; i<points.size() && i*2+1<input.d.size(); ++i) {
            input.at(0, i*2) = normalize(points[i].first, x_min, x_max);
            input.at(0, i*2+1) = normalize(points[i].second, y_min, y_max);
        }
        Matrix Z1=add_bias(multiply(input,W1),B1), A1=relu(Z1);
        Matrix Z2=add_bias(multiply(A1,W2),B2), A2=relu(Z2);
        Matrix Z3=add_bias(multiply(A2,W3),B3);
        double m = denormalize(Z3.at(0,0), mb_min, mb_max);
        double b = denormalize(Z3.at(0,1), mb_min, mb_max);
        return {m, b};
    }
};

constexpr int SCREEN_WIDTH = 640; constexpr int SCREEN_HEIGHT = 480;
class VgaSimulator{SDL_Window*w=nullptr;SDL_Renderer*r=nullptr;SDL_Texture*t=nullptr;std::vector<uint32_t>fb;public:VgaSimulator(){if(SDL_Init(SDL_INIT_VIDEO)<0)throw std::runtime_error("SDL fail");w=SDL_CreateWindow("VGA Sim",100,100,SCREEN_WIDTH,SCREEN_HEIGHT,SDL_WINDOW_SHOWN);r=SDL_CreateRenderer(w,-1,SDL_RENDERER_ACCELERATED);t=SDL_CreateTexture(r,SDL_PIXELFORMAT_ARGB8888,SDL_TEXTUREACCESS_STREAMING,SCREEN_WIDTH,SCREEN_HEIGHT);fb.resize(SCREEN_WIDTH*SCREEN_HEIGHT,0);}~VgaSimulator(){SDL_DestroyTexture(t);SDL_DestroyRenderer(r);SDL_DestroyWindow(w);SDL_Quit();}void clear(uint32_t c){std::fill(fb.begin(),fb.end(),c);}void draw_pixel(int x,int y,uint32_t c){if(x>=0&&x<SCREEN_WIDTH&&y>=0&&y<SCREEN_HEIGHT)fb[y*SCREEN_WIDTH+x]=c;}void present(){SDL_UpdateTexture(t,NULL,fb.data(),SCREEN_WIDTH*sizeof(uint32_t));SDL_RenderClear(r);SDL_RenderCopy(r,t,NULL,NULL);SDL_RenderPresent(r);}bool process_events(){SDL_Event e;while(SDL_PollEvent(&e)!=0)if(e.type==SDL_QUIT)return false;return true;}};
void draw_point_on_vga(VgaSimulator&v,int cx,int cy,uint32_t c){for(int y=cy-2;y<=cy+2;++y)for(int x=cx-2;x<=cx+2;++x)v.draw_pixel(x,y,c);}
void draw_line_bresenham(VgaSimulator&v,int x0,int y0,int x1,int y1,uint32_t c){int dx=std::abs(x1-x0),sx=x0<x1?1:-1,dy=-std::abs(y1-y0),sy=y0<y1?1:-1,err=dx+dy,e2;for(;;){v.draw_pixel(x0,y0,c);if(x0==x1&&y0==y1)break;e2=2*err;if(e2>=dy){err+=dy;x0+=sx;}if(e2<=dx){err+=dx;y0+=sy;}}}
struct CoordMapper{double xmin,xmax,ymin,ymax;CoordMapper(const std::vector<std::pair<double,double>>&p,double m,double b){if(p.empty()){xmin=-10;xmax=10;ymin=-10;ymax=10;}else{xmin=p[0].first;xmax=p[0].first;ymin=p[0].second;ymax=p[0].second;for(const auto&pt:p){xmin=std::min(xmin,pt.first);xmax=std::max(xmax,pt.first);ymin=std::min(ymin,pt.second);ymax=std::max(ymax,pt.second);}}if(!p.empty()){double y1=m*xmin+b,y2=m*xmax+b;ymin=std::min({ymin,y1,y2});ymax=std::max({ymax,y1,y2});}double xp=(xmax-xmin)*0.1+1.0,yp=(ymax-ymin)*0.1+1.0;xmin-=xp;xmax+=xp;ymin-=yp;ymax+=yp;}std::pair<int,int>world_to_screen(double wx,double wy){double ww=xmax-xmin,wh=ymax-ymin;if(std::abs(ww)<1e-6)ww=1.0;if(std::abs(wh)<1e-6)wh=1.0;return{static_cast<int>((wx-xmin)/ww*SCREEN_WIDTH),static_cast<int>(SCREEN_HEIGHT-((wy-ymin)/wh*SCREEN_HEIGHT))};}};
std::mutex g_data_mutex; std::vector<std::pair<double,double>> g_user_points; std::optional<std::pair<double,double>>g_new_point;
void input_thread_func(){std::string line;while(true){std::cout<<"\nВведите точку (x,y) > ";if(!std::getline(std::cin,line)||line=="stop"||line=="exit")break;std::stringstream ss(line);double x,y;char c;if(!(ss>>x>>c>>y)||c!=','){std::cerr<<"Ошибка ввода. Формат 'x,y'."<<std::endl;continue;}std::lock_guard<std::mutex>lock(g_data_mutex);g_new_point={x,y};}}

int main(int argc, char* argv[]) {
    try {
        VgaSimulator vga; NeuroProcessor neuro_processor;
        neuro_processor.load_weights("network_weights.txt");
        std::thread input_thread(input_thread_func); input_thread.detach();
        double m=0.0, b=0.0; bool running=true;
        while(running){
            if(!vga.process_events())running=false;
            bool needs_recalc=false;
            {std::lock_guard<std::mutex>lock(g_data_mutex);if(g_new_point){g_user_points.push_back(*g_new_point);g_new_point.reset();needs_recalc=true;}}
            if(needs_recalc&&g_user_points.size()>=2){
                std::vector<std::pair<double,double>>points_for_nn;
                {std::lock_guard<std::mutex>lock(g_data_mutex);auto start=g_user_points.size()>3?g_user_points.end()-3:g_user_points.begin();points_for_nn=std::vector<std::pair<double,double>>(start,g_user_points.end());}
                std::tie(m,b)=neuro_processor.process(points_for_nn);
                std::cout << "Новые коэффициенты (вычислены сетью): m=" << std::fixed << std::setprecision(4) << m << ", b=" << b << std::endl;
            }
            std::vector<std::pair<double,double>>points_copy;{std::lock_guard<std::mutex>lock(g_data_mutex);points_copy=g_user_points;}
            CoordMapper mapper(points_copy,m,b);vga.clear(0xFF101010);
            auto origin=mapper.world_to_screen(0,0);
            draw_line_bresenham(vga,0,origin.second,SCREEN_WIDTH-1,origin.second,0xFF404040);
            draw_line_bresenham(vga,origin.first,0,origin.first,SCREEN_HEIGHT-1,0xFF404040);
            for(const auto&p:points_copy){auto[sx,sy]=mapper.world_to_screen(p.first,p.second);draw_point_on_vga(vga,sx,sy,0xFF00A0FF);}
            if(points_copy.size()>=2){auto p1=mapper.world_to_screen(mapper.xmin,m*mapper.xmin+b);auto p2=mapper.world_to_screen(mapper.xmax,m*mapper.xmax+b);draw_line_bresenham(vga,p1.first,p1.second,p2.first,p2.second,0xFFFF4040);}
            vga.present();SDL_Delay(16);
        }
    }catch(const std::exception& e){std::cerr<<"Критическая ошибка: "<<e.what()<<std::endl;return 1;}
    return 0;
}
```
#### Компиляция
```
g++ visualize_approximation.cpp -o visualize_approximation.exe -std=c++17 -lSDL2 -mconsole -finput-charset=CP1251 -fexec-charset=CP866
```
#### Запуск
Программа принимает на вход координаты точек и даёт на выход получившиеся коэффициенты линейной функции.
### 4. Пояснение
#### Архитектура

*   **Тип:** Полносвязная нейронная сеть прямого распространения (Feedforward Neural Network) или многослойный перцептрон (Multi-Layer Perceptron, MLP).
*   **Структура:** Сеть состоит из входного слоя, двух скрытых слоев и одного выходного слоя. Это классическая архитектура для задач регрессии, где по входным данным нужно предсказать непрерывные значения.

#### Количество слоёв

*   **Всего слоёв с нейронами:** 4 (1 входной, 2 скрытых, 1 выходной).
*   **Всего обучаемых слоёв (слоёв с весами):** 3. Каждая пара (матрица весов `W` и вектор смещений `B`) соответствует одному обучаемому слою. В коде это `W1, B1`, `W2, B2` и `W3, B3`.

#### Количество нейронов

*   **Входной слой:** **6 нейронов**.
    *   Это определяется константой `INPUT_SIZE`, которая вычисляется как `NUM_POINTS * 2`. Поскольку `NUM_POINTS = 3`, сеть принимает на вход координаты трёх точек (3 точки * 2 координаты (x, y) = 6 входов).
*   **Первый скрытый слой:** **32 нейрона** (константа `HIDDEN1_SIZE`).
*   **Второй скрытый слой:** **16 нейронов** (константа `HIDDEN2_SIZE`).
*   **Выходной слой:** **2 нейрона** (константа `OUTPUT_SIZE`).
    *   Сеть предсказывает два значения: коэффициент наклона `m` и свободный член `b` для прямой `y = m*x + b`.

#### Количество эпох

*   **Понятие "эпоха" здесь не совсем применимо.** Эпоха — это один полный проход по всему набору данных. В этом коде данные генерируются "на лету" на каждой итерации, поэтому у нас нет фиксированного набора данных.
*   Вместо этого используется **количество шагов (итераций) обучения**.
*   **Количество шагов:** **80 000**
    *   Это задано переменной `int steps=80000;`. На каждом шаге генерируется новый батч данных и выполняется одна итерация градиентного спуска.

#### Активация

*   **Скрытые слои:** **ReLU** (Rectified Linear Unit).
    *   Это видно по вызовам функции `apply_relu` после вычисления `Z1` и `Z2`:
      ```cpp
      Matrix Z1=add_bias(multiply(X_batch,W1),B1), A1=apply_relu(Z1);
      Matrix Z2=add_bias(multiply(A1,W2),B2),     A2=apply_relu(Z2);
      ```
    *   Сама функция `apply_relu` реализует `max(0, x)`.

*   **Выходной слой:** **Линейная (или тождественная)**.
    *   К выходу последнего слоя (`Z3`) не применяется никакая функция активации.
      ```cpp
      Matrix Z3=add_bias(multiply(A2,W3),B3), Y_pred=Z3;
      ```
    *   Это стандартный подход для задач регрессии, где выходные значения могут быть любыми действительными числами.

#### Функция ошибки

*   **Среднеквадратичная ошибка (Mean Squared Error, MSE)**.
*   Это можно определить по нескольким признакам:
    1.  **Вычисление ошибки:** `error.d[i]=Y_pred.d[i]-Y_batch.d[i];`. Это разница между предсказанным и истинным значением.
    2.  **Начало обратного распространения:** `Matrix dZ3=error;`. Градиент MSE по выходу сети как раз и является этой разницей (`prediction - true_value`), умноженной на константу.
    3.  **Логирование потерь:** `loss+=e*e; ... std::cout << "Потери: " << loss/N << std::endl;`. Здесь `loss` — это сумма квадратов ошибок, а `loss/N` (где `N` — размер батча) — это и есть среднеквадратичная ошибка для данного батча.
